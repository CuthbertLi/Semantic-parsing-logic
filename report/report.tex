\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx, fancyhdr, amsmath, amssymb, amsthm, subfig}
\usepackage{indentfirst}
\usepackage{pdfpages}
\usepackage{dirtree}
\usepackage{lastpage, hyperref}
% \usepackage{hyperref, url}

% \pagestyle{fancy}
% \fancyhf{}
% \rhead{Page \thepage\ of \pageref{LastPage}}
\title{Semantic Parsing for First-order Logic}
\author{
	Li Dinghong\\
	SIST, Shanghaitech University\\
	27663262\\
	\{lidh1, liujd, wudi\}@shanghaitech.edu.cn
}

\begin{document}
% abstract and beginnings
{
	\newpage
	\maketitle

	\textbf{Abstract} - {In this paper, we present a generic model for semantic parsing. It takes context-free grammar (CFG) parsing trees as input, and outputs first-order logic (FOL) expressions. We get FLO expressions by traversing CFG parsing trees. }

	\vspace{5pt}
	\textbf{\emph{Keywords:}} {}

	\tableofcontents
}

\section{Introduction}{
	\subsection{Background and Motivating Applications}{
		The Goal of semantic parsing is to formalize natural languages. That is, translating texts in natural languages into formal languges, like first-order logic (FOL) or higher-order logic. 

		% Motivating applications: 

		% Formal verification of programming languages

		% Formal verification of computer security

		% Automated theorem proving in mathematics

		Semantic parsing can be applied in a wide range of fields both in computer science and outside computer science. Computer security of a software, for instance, cannot be confirmed until it is studied formally and mathematically. Empirical methods like fuzzing are not so reliable because they lack solid foundations. Thus, semantic parsing is necessary. 

		Formal verification of programming languages also require to formalize user manuals or official guides, which are typically written in natural languages. Most of these tasks are done by hand now. This is tedious and erroneous. 

		Semantic parsing can also help to prove mathematical theorems as well as to check their proofs automatically. Output of a semantic parser is in logical forms, so it can be accepted by computers. Computers can then finish later tasks by inference. 
	}

	\subsection{Our Contributions}{
		Thanks to parsing trees, our model is highly generic. We can easily reuse it for a different natural language by simply defining new rules. 

		\cite{matsuzaki}

		We succeed to build a binding and scoping structures based on parsing trees. 
	}
}

\section{Methods}{
	\subsection{Grammars and Parsing Tree}{ 
		\cite{su}

		According to Chomsky hierarchy, formal grammars can be classified as regular grammar, context-free grammar (CFG), context-sensitive grammar and recursive enumerable grammar. 

		Context-sensitive grammar is computationally intractable and recursive enumerable grammar is even undecideable, although they more expressive and more powerful. Besides, regular grammar is obviously too restrictive. Therefore, we can choose either context-free grammar or one of its subsets, dependency grammar. 

		We have first considered dependency grammar, since dependency trees seem to preserve enough information we need. Meanwhile, dependency trees are much smaller than CFG parsing trees. However, we have found it insufficient. Dependency parsing trees cannot provide groups of words within a sentence, but this information is indispensible for first-order logic. For instance, 

		Therefore, we choose CFG parsing trees as our input. We can get CFG parsing trees from texts in natural languages, and then generate FOL expressions from the trees. 
	}

	\subsection{Recursive Definition of FOL}{
		% Terms

		% The set of terms is inductively defined by the following rules:

		% Variables. Any variable is a term.
		% Functions. Any expression f(t1,...,tn) of n arguments (where each argument ti is a term and f is a function symbol of valence n) is a term. In particular, symbols denoting individual constants are nullary function symbols, and are thus terms.

		% Only expressions which can be obtained by finitely many applications of rules 1 and 2 are terms. For example, no expression involving a predicate symbol is a term.
		% Formulas

		% The set of formulas (also called well-formed formulas[9] or WFFs) is inductively defined by the following rules:

		% Predicate symbols. If P is an n-ary predicate symbol and t1, ..., tn are terms then P(t1,...,tn) is a formula.
		% Equality. If the equality symbol is considered part of logic, and t1 and t2 are terms, then t1 = t2 is a formula.
		% Negation. If φ is a formula, then ¬ {\displaystyle \lnot } \lnot φ is a formula.
		% Binary connectives. If φ and ψ are formulas, then (φ → {\displaystyle \rightarrow } \rightarrow ψ) is a formula. Similar rules apply to other binary logical connectives.
		% Quantifiers. If φ {\displaystyle \varphi } \varphi is a formula and x is a variable, then ∀ x φ {\displaystyle \forall x\varphi } \forall x\varphi (for all x, φ {\displaystyle \varphi } \varphi holds) and ∃ x φ {\displaystyle \exists x\varphi } \exists x\varphi (there exists x such that φ {\displaystyle \varphi } \varphi ) are formulas.

		% Only expressions which can be obtained by finitely many applications of rules 1–5 are formulas. The formulas obtained from the first two rules are said to be atomic formulas.
	}
}

\section{Parsing Tree Traversal}{
	% Supervised learning and neural networks are costly and unreliable for semantic parsing

	% Although supervised learning and neural networks are amazingly successful in natural language processing, advances in semantic parsing are relatively little. 

	% Since CFG parsing trees are deterministic and informative enough, rules that are needed to add are very limited. 

	Although supervised learning and neural networks are amazingly successful in natural language processing, advances in semantic parsing are relatively little. Most existing methods of semantic parsing are highly restrictive in fields or grammars. 

	% Therefore, we develop a symbolic approach for semantic parsing, which is computationally less complex and more reliable and predictable than learning algorithms. 

	Therefore, we develop a symbolic approach for semantic parsing, which is computationally less complex and more reliable and predictable than learning algorithms. We generate first-order logic expressions by traversing CFG parsing trees and defining a small number of rules manually. Since CFG parsing trees are deterministic and informative enough, rules we need to add are very limited. 

	\subsection{Generation of FOL}{
		% Generation of negation and equality is quite simple. 

		% However, generation of binary connectives and quantifiers is not trivial. 

		% We have to avoid common mistakes like using $\wedge $ as the main connective with $\forall $, or using $\wedge $ as the main connective with $\Rightarrow $ as the main connective with $\exists $. 

		% Predicate symbols and functions are ubiquitous in first-order logic. To tackle disambiguation as well as binding and scoping more easily, however, we use them even more. 

		Now we can generate first-order logic expressions by inductive formation rules. 

		Generation of negation and equality is quite simple. However, generation of binary connectives and quantifiers is not trivial. We have to avoid common mistakes like using $\wedge $ as the main connective with $\forall $, or using $\wedge $ as the main connective with $\Rightarrow $ as the main connective with $\exists $. 

		We thus add some rules to avoid this. Whenever the output expression is started with $\exists $, we have to generate $\wedge $ instead of $\Rightarrow$. 

		Predicate symbols and functions are ubiquitous in first-order logic. To tackle disambiguation as well as binding and scoping more easily, however, we use them even more. We define a new variable name when we meet a constant, by defining a constant function for it. 
	}

	\subsection{Binding and Scoping}{
		% Binding is not challenging for a single sentence. However, determining scope of variables is not even possible without the help of parsing trees. 

		Binding is not challenging for a single sentence. However, determining scope of variables is not even possible without the help of parsing trees. 

		% Rule: A variable lives within the tree rooted by the closest non-trivial NP parent

		Our rule is that a variable lives within the tree rooted by the closest non-trivial NP parent. 
	}
}

\section{Results}{}

\section{Conclusion}{
	% We succeed to build a generic and reliable model of semantic parsing based on CFG parsing trees

	% Symbolic methods are possible to work well for semantic parsing as long as a correct CFG parsing tree is given

	% We succeed to build a binding and scoping structure based on CFG parsing trees
}

\bibliographystyle{unsrt}
\bibliography{report}
\citation

\end{document}